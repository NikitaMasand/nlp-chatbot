{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "chatbots_file = open('chatbot.txt','r',errors = 'ignore')\n",
    "content = chatbots_file.read()\n",
    "content = content.lower()\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "sentence_tokens = nltk.sent_tokenize(content)\n",
    "word_tokens = nltk.word_tokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_tokens(tokens):\n",
    "    return[lemmer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punct_dict = dict((ord(punct),None) for punct in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_normalize(text):\n",
    "    return lem_tokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\",\"hi\",\"what's up\",\"sup\",\"hey\")\n",
    "GREETING_RESPONSES = (\"hi\",\"hey\",\"*nods*\",\"hi there\",\"hello\",\"I am glad! You are talking to me\")\n",
    "\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response = ''\n",
    "    sentence_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer = lem_normalize, stop_words = 'english')\n",
    "    tfidf = TfidfVec.fit_transform(sentence_tokens)\n",
    "    \n",
    "    values = cosine_similarity(tfidf[-1],tfidf)\n",
    "    idx = values.argsort()[0][-2]\n",
    "    flat = values.flatten()\n",
    "    flat.sort()\n",
    "    \n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response = robo_response + \"I am sorry! I don't understand you\"\n",
    "    else:\n",
    "        robo_response = robo_response + sentence_tokens[idx]\n",
    "    return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chiku: My name is chiku. I will answer your queries about chatbots. If you want to exit, type bye!\n",
      "hi\n",
      "Chiku: hi there\n",
      "what is a turing machine?\n",
      "Chiku: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\chrome downloads\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background\n",
      "in 1950, alan turing's famous article \"computing machinery and intelligence\" was published, which proposed what is now called the turing test as a criterion of intelligence.\n",
      "who are the classic historic early chatbots?\n",
      "Chiku: \n",
      "development\n",
      "the classic historic early chatbots are eliza (1966) and parry (1972).more recent notable programs include a.l.i.c.e., jabberwacky and d.u.d.e (agence nationale de la recherche and cnrs 2006).\n",
      "ok thank you\n",
      "Chiku: \n",
      "I am sorry! I don't understand you\n",
      "thank you\n",
      "Chiku: You are welcome\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "print(\"Chiku: My name is chiku. I will answer your queries about chatbots. If you want to exit, type bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you'):\n",
    "            flag = False\n",
    "            print(\"Chiku: You are welcome\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"Chiku: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"Chiku: \")\n",
    "                print(response(user_response))\n",
    "                sentence_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"Chiku: Bye, take care! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
